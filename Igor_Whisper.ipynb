{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RjIp6MCDV8fE",
        "-OU1OwdYfvEu"
      ],
      "gpuType": "T4",
      "mount_file_id": "1ry4_ne80C9hxR_lVrUhIpJFshV8ud_Pb",
      "authorship_tag": "ABX9TyO4yEpxg7D09tGL10Zne7L/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorspurs/00-igorspurs/blob/main/Igor_Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalar whisper"
      ],
      "metadata": {
        "id": "RjIp6MCDV8fE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VzkpVKQdVvN4",
        "outputId": "0ba54446-7a30-4bf8-a7fe-980ebb5fee30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-j9w0h1rx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-j9w0h1rx\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "65 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 65 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WkUxTWqwXnxN",
        "outputId": "a4a62a06-de78-4dcd-b612-2059532988a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=ad750c8937d98cbdfc3a21a825067f027db7c2dffc699592e7790b0d44505aa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS: Ok, instalados whisper e ffmpeg"
      ],
      "metadata": {
        "id": "kgya7S_bjc0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verificar se o whisper está instalado"
      ],
      "metadata": {
        "id": "sXqJv67fpiBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Verificando a versão do Whisper instalado\n",
        "print(\"Whisper instalado com sucesso. Versão:\", whisper.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sMimJ1JGpaH8",
        "outputId": "753dbcd5-4591-4f54-cabf-3a8e90c8fb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper instalado com sucesso. Versão: 20240930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transcrever Padrão\n",
        "\n"
      ],
      "metadata": {
        "id": "RqtU9jPUWnhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Posso fazer upload direto na pasta do lado esquerdo, vou jogar 03 arquivos aleatorios para teste\n"
      ],
      "metadata": {
        "id": "m9wRiO0TiDZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcrever\n",
        "!whisper \"teste 3.mp4\" --model tiny --language Portuguese --output_format txt\n",
        "#teste.mp4 é um arquivo que ta do lado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0u6UWYgOcZ0I",
        "outputId": "aef7e7c7-59b8-421b-9a30-911da452d84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 72.1M/72.1M [00:30<00:00, 2.51MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'teste 3.mp4', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n",
            "Skipping teste 3.mp4 due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#voltar para a pasta home do google colab\n",
        "%cd '/content'"
      ],
      "metadata": {
        "id": "O_-Izgcim2LM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ade0dc-92eb-4615-c86b-3ae3d73013cf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Se eventualmente eu tiver usado o whisper no Google Drive, o comando não vai dar certo sem antes trazer o whisper para a pasta home"
      ],
      "metadata": {
        "id": "qXISrFfEm3h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelos carregados\n",
        "\n",
        "####tiny\n",
        "####base\n",
        "####small\n",
        "####turbo\n",
        "\n",
        "- vou optar mais pelo **TURBO**, mais rápido e com qualidade boa\n",
        "\n",
        "- mudar cf mp3 ou mp4\n",
        "\n",
        "##Template do comando whisper para 01 arquivo\n",
        "> !whisper \"teste 3.mp4\" --model tiny --language Portuguese --output_format txt\n",
        "\n",
        "##Template do comando whisper para mais de um arquivo\n",
        "(nao usar o language portuguese - separar os arquivos)\n",
        "> !whisper \"teste 3.mp4\" \"teste 2.mp4\" --model turbo\n",
        "\n",
        "- Pode usar uma linha de comando mandando formatos diferentes como mp3 e mp4 ao mesmo tempo"
      ],
      "metadata": {
        "id": "QAA_ggR5h0xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Templates\n",
        "\n",
        "### 01 Comando whisper para 01 arquivo\n",
        "\n",
        ">!whisper \"teste 3.mp4\" --model tiny --language Portuguese --output_format txt\n",
        "\n",
        "### Comando whisper para mais de um arquivo\n",
        "\n",
        "(nao usar o language portuguese - separar os arquivos)\n",
        "\n",
        "> !whisper \"teste 3.mp4\" \"teste 2.mp4\" --model turbo\n",
        "\n",
        "- Pode usar uma linha de comando mandando formatos diferentes como mp3 e mp4 ao mesmo tempo\n"
      ],
      "metadata": {
        "id": "KTZQm-qvqQJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outras transcrições"
      ],
      "metadata": {
        "id": "hS3eTq9tnWNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "usar esse codigo como pool para novas transcrições, ao inves de bagunçar e escrever varias linhas de codigo lá embaixo"
      ],
      "metadata": {
        "id": "zdF2F6hMnozO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcrever\n",
        "\n"
      ],
      "metadata": {
        "id": "Ggw1XdNzncGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Carregar os moutros modelos"
      ],
      "metadata": {
        "id": "-OU1OwdYfvEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forcei os outros modelos: small, base, turbo, para deixar pre-carregado para futuras execuçoes"
      ],
      "metadata": {
        "id": "PG9X_njKf3iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"teste 3.mp4\" --model small --language Portuguese --output_format txt\n",
        "#carregar o modelo small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EbJ15aoceH5N",
        "outputId": "243acd2a-fc5b-4cef-b8ee-18a221dfde12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:05<00:00, 89.0MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'teste 3.mp4', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n",
            "Skipping teste 3.mp4 due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"teste 3.mp4\" --model base --language Portuguese --output_format txt\n",
        "#vou rodar de novo esse comando para carregar o modelo base\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QVeS6djadf9T",
        "outputId": "10c04426-e31e-4a0b-dc67-c3c73a7ae1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 46.4MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'teste 3.mp4', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n",
            "Skipping teste 3.mp4 due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"teste 3.mp4\" --model turbo --language Portuguese --output_format txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HHLWOKAfe2AB",
        "outputId": "2b2d9680-95cd-4750-8fb0-7d49225879f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 1.51G/1.51G [00:21<00:00, 75.2MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'teste 3.mp4', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n",
            "Skipping teste 3.mp4 due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Montar Google Drive (e transcrever)"
      ],
      "metadata": {
        "id": "fR8Ip6MKWASf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS: já está montado, nao precisa clicar nesse de montar, apenas no %cd da pasta"
      ],
      "metadata": {
        "id": "gRH7F1OLrvX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Google Drive disponivel aqui\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lzBeyzjbWIUT",
        "outputId": "8fadfc6d-91a7-4bdc-e357-072fafa58d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pasta especifica no drive\n",
        "%cd \"/content/drive/MyDrive/Igor Whisper/Pasta 01\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NCYk6O3XkVVm",
        "outputId": "1acc860d-b97a-49cb-d910-45074fb8837f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Igor Whisper/Pasta 01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Usar a pasta 01 para esse nó, vou criar uma pasta 02 para tentar transcrever todos os arquivos de uma vez;\n",
        "- Apos montar essa pasta posso usar o comando whisper c/ nomes de arquivos que ja estejam na pasta;\n",
        "- Para transcrever vários arquivos pelo comando de nome posso usar o xed e o ctrl + H"
      ],
      "metadata": {
        "id": "DuHA9PyGmd21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transcrever arquivos do google drive"
      ],
      "metadata": {
        "id": "qvQbagTyq-o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcrever\n",
        "!whisper \"teste 3.mp4\" --model tiny --language Portuguese --output_format txt\n",
        "#teste.mp4 é um arquivo nomeado na pasta do google drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e43f44c7-adc2-4681-8ea5-60e3a799706f",
        "id": "DjjHZqFdrQOS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'teste 3.mp4', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n",
            "Skipping teste 3.mp4 due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "teste 3.mp4: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se eu quiser voltar para a pasta raiz do google drive, eu clico aqui"
      ],
      "metadata": {
        "id": "6YR5l08CrXuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#voltar para a pasta home do google colab\n",
        "%cd '/content'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WLMJVPdEkuYB",
        "outputId": "ab636cf9-22bf-4b7e-fc8e-cbca376199f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transcrever arquivos upando do PC"
      ],
      "metadata": {
        "id": "xoUnwIaa6_gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pasta home do google colab\n",
        "%cd '/content'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8ee9ae0f-6c46-4c20-c949-fd4fc877cb78",
        "id": "CD7N4CWo8tjv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# 1. Cria a pasta \"uploads\" no Colab, caso ainda não exista\n",
        "upload_dir = \"uploads\"\n",
        "if not os.path.exists(upload_dir):\n",
        "    os.makedirs(upload_dir)\n",
        "\n",
        "# 2. Faz o upload do arquivo do PC para a pasta específica\n",
        "print(f\"Faça o upload do arquivo para a pasta '{upload_dir}':\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 3. Move os arquivos carregados para a pasta \"uploads\"\n",
        "for file_name in uploaded.keys():\n",
        "    os.rename(file_name, os.path.join(upload_dir, file_name))\n",
        "\n",
        "# Lista os arquivos na pasta \"uploads\"\n",
        "print(f\"Arquivos atualmente na pasta '{upload_dir}':\")\n",
        "os.listdir(upload_dir)\n",
        "\n",
        "# 4. Executa o Whisper em cada arquivo da pasta\n",
        "for file_name in os.listdir(upload_dir):\n",
        "    input_file = os.path.join(upload_dir, file_name)\n",
        "    print(f\"Transcrevendo arquivo: {file_name}\")\n",
        "\n",
        "    # Comando do Whisper - Salva a transcrição diretamente na pasta \"uploads\"\n",
        "    !whisper \"{input_file}\" --model tiny --language Portuguese --output_dir \"{upload_dir}\" --output_format txt\n",
        "\n",
        "    # Deleta o arquivo de mídia após a transcrição\n",
        "    os.remove(input_file)\n",
        "    print(f\"Arquivo de mídia deletado: {file_name}\")\n",
        "\n",
        "# 5. Compacta os arquivos transcritos\n",
        "zip_file = \"transcricoes.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "    for file_name in os.listdir(upload_dir):\n",
        "        file_path = os.path.join(upload_dir, file_name)\n",
        "        zipf.write(file_path, arcname=file_name)  # Adiciona ao ZIP\n",
        "        os.remove(file_path)  # Exclui o arquivo após adicioná-lo ao ZIP\n",
        "        print(f\"Arquivo transcrito deletado: {file_name}\")\n",
        "\n",
        "# Confirmação de que o ZIP foi salvo\n",
        "print(f\"Arquivo ZIP criado: {zip_file}\")\n",
        "\n",
        "# 6. Faz o download do arquivo ZIP\n",
        "print(f\"Iniciando o download do arquivo ZIP: {zip_file}\")\n",
        "files.download(zip_file)\n",
        "\n",
        "# 7. Exclusão manual (opcional)\n",
        "print(f\"O arquivo ZIP '{zip_file}' não será excluído automaticamente.\")\n",
        "print(\"Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "collapsed": true,
        "id": "H22cd3wQFNW4",
        "outputId": "9aa36029-126e-4f4a-a186-119046af6dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faça o upload do arquivo para a pasta 'uploads':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21a7736b-419c-49eb-9fbf-207525c6c157\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21a7736b-419c-49eb-9fbf-207525c6c157\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos atualmente na pasta 'uploads':\n",
            "Arquivo ZIP criado: transcricoes.zip\n",
            "Iniciando o download do arquivo ZIP: transcricoes.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_11c8d4b2-77e2-4d2e-b362-bb5bef4535d7\", \"transcricoes.zip\", 22)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O arquivo ZIP 'transcricoes.zip' não será excluído automaticamente.\n",
            "Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outras transcrições"
      ],
      "metadata": {
        "id": "vCcsR_97HhHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usar esse aqui para não destruir o codigo acima"
      ],
      "metadata": {
        "id": "c0CRxsB2HkfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcrever"
      ],
      "metadata": {
        "id": "iMVu-e5pHp-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Excluir zip remanescente"
      ],
      "metadata": {
        "id": "bFcnuYBPDEHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.remove('transcricoes.zip')"
      ],
      "metadata": {
        "id": "uJownxRKDC5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, tudo certo nessa também\n",
        "\n",
        "Módulo finalizado"
      ],
      "metadata": {
        "id": "MTgZGwKuGB2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modulo de midias da internet"
      ],
      "metadata": {
        "id": "IBLVOHC_G4uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como p.ex. videos do youtube"
      ],
      "metadata": {
        "id": "sAB9j8nbH_mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se o yt-dlp está instalado e instala apenas se necessário\n",
        "try:\n",
        "    import yt_dlp\n",
        "    print(\"yt-dlp já está instalado.\")\n",
        "except ImportError:\n",
        "    print(\"yt-dlp não encontrado, instalando agora...\")\n",
        "    !pip install yt-dlp --quiet\n",
        "    import yt_dlp\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# 1. Configurações iniciais\n",
        "upload_dir = \"uploads\"\n",
        "if not os.path.exists(upload_dir):\n",
        "    os.makedirs(upload_dir)\n",
        "\n",
        "# 2. Função para baixar vídeos do YouTube usando yt-dlp\n",
        "def baixar_video_youtube(url, download_dir):\n",
        "    try:\n",
        "        # Configurações do yt-dlp\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',  # Melhor qualidade\n",
        "            'outtmpl': os.path.join(download_dir, '%(title)s.%(ext)s'),  # Nome do arquivo\n",
        "        }\n",
        "\n",
        "        # Baixa o vídeo usando yt-dlp\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            result = ydl.extract_info(url, download=True)\n",
        "            file_path = os.path.join(download_dir, f\"{result['title']}.mp4\")\n",
        "            print(f\"Vídeo baixado: {file_path}\")\n",
        "            return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao baixar o vídeo: {e}\")\n",
        "        return None\n",
        "\n",
        "# 3. Insere múltiplas URLs\n",
        "urls = input(\"Insira as URLs dos vídeos (separadas por vírgula): \").split(',')\n",
        "\n",
        "# 4. Baixa os vídeos das URLs fornecidas\n",
        "for url in urls:\n",
        "    url = url.strip()  # Remove espaços extras\n",
        "    video_path = baixar_video_youtube(url, upload_dir)\n",
        "\n",
        "    if video_path:\n",
        "        print(f\"Vídeo salvo em: {video_path}\")\n",
        "    else:\n",
        "        print(f\"Erro ao processar o URL: {url}\")\n",
        "\n",
        "# 5. Processa os arquivos na pasta \"uploads\"\n",
        "for file_name in os.listdir(upload_dir):\n",
        "    input_file = os.path.join(upload_dir, file_name)\n",
        "    print(f\"Transcrevendo arquivo: {file_name}\")\n",
        "\n",
        "    # Comando do Whisper\n",
        "    !whisper \"{input_file}\" --model tiny --language Portuguese --output_dir \"{upload_dir}\" --output_format txt\n",
        "\n",
        "    # Deleta o arquivo de mídia após a transcrição\n",
        "    os.remove(input_file)\n",
        "    print(f\"Arquivo de mídia deletado: {file_name}\")\n",
        "\n",
        "# 6. Compacta as transcrições em um arquivo ZIP\n",
        "zip_file = \"transcricoes.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "    for file_name in os.listdir(upload_dir):\n",
        "        file_path = os.path.join(upload_dir, file_name)\n",
        "        zipf.write(file_path, arcname=file_name)  # Adiciona ao ZIP\n",
        "        os.remove(file_path)  # Exclui o arquivo após adicioná-lo ao ZIP\n",
        "        print(f\"Arquivo transcrito deletado: {file_name}\")\n",
        "\n",
        "# 7. Faz o download do arquivo ZIP\n",
        "print(f\"Iniciando o download do arquivo ZIP: {zip_file}\")\n",
        "files.download(zip_file)\n",
        "\n",
        "# 8. Exclusão manual do ZIP (opcional)\n",
        "print(f\"O arquivo ZIP '{zip_file}' não será excluído automaticamente.\")\n",
        "print(\"Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "15bO6auWIvvD",
        "outputId": "979db632-2247-49c1-e732-ce67621d0148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yt-dlp não encontrado, instalando agora...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInsira as URLs dos vídeos (separadas por vírgula): https://www.youtube.com/watch?v=RLH3B1UkQaM\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=RLH3B1UkQaM\n",
            "[youtube] RLH3B1UkQaM: Downloading webpage\n",
            "[youtube] RLH3B1UkQaM: Downloading ios player API JSON\n",
            "[youtube] RLH3B1UkQaM: Downloading tv player API JSON\n",
            "[youtube] RLH3B1UkQaM: Downloading player 0b866fa6\n",
            "[youtube] RLH3B1UkQaM: Downloading m3u8 information\n",
            "[info] RLH3B1UkQaM: Downloading 1 format(s): 247+251\n",
            "[download] Destination: uploads/NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.f247.webm\n",
            "[download] 100% of   30.13MiB in 00:00:02 at 10.93MiB/s  \n",
            "[download] Destination: uploads/NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.f251.webm\n",
            "[download] 100% of    4.55MiB in 00:00:00 at 6.84MiB/s   \n",
            "[Merger] Merging formats into \"uploads/NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.webm\"\n",
            "Deleting original file uploads/NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.f251.webm (pass -k to keep)\n",
            "Deleting original file uploads/NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.f247.webm (pass -k to keep)\n",
            "Vídeo baixado: uploads/NÃO TEM QUE PENSAR NA F1: Castroneves MANDA REAL sobre DRUGO na Indy | Felipe segue RESERVA DA ASTON.mp4\n",
            "Vídeo salvo em: uploads/NÃO TEM QUE PENSAR NA F1: Castroneves MANDA REAL sobre DRUGO na Indy | Felipe segue RESERVA DA ASTON.mp4\n",
            "Transcrevendo arquivo: NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.webm\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:06.000]  E obviamente ali, eu não poderia deixar de...\n",
            "[00:06.000 --> 00:08.000]  E ausenta aqui, de Fladding.\n",
            "[00:08.000 --> 00:11.000]  De dois aspectos, como que você vê?\n",
            "[00:11.000 --> 00:15.000]  A nova leva de brasileiros aí, especialmente os irmãos futepaldi.\n",
            "[00:15.000 --> 00:18.000]  E o seu grande amigo, Tony Canando,\n",
            "[00:18.000 --> 00:22.000]  ganhando, cada vez mais força dentro ali da McLaren.\n",
            "[00:24.000 --> 00:27.000]  Pois é, eu tô vendo ele, cada vez mais forte na McLaren,\n",
            "[00:27.000 --> 00:31.000]  é um cara que, então, é um avião noção dos amigos há muito tempo.\n",
            "[00:31.000 --> 00:34.000]  E eu sempre admirei o Tony nesse sentido.\n",
            "[00:34.000 --> 00:37.000]  A gente teve várias conversas, inclusive,\n",
            "[00:37.000 --> 00:41.000]  em ser parceiro e equipes, e eu já tinha otado,\n",
            "[00:41.000 --> 00:45.000]  que tem conhecimento nessa área.\n",
            "[00:45.000 --> 00:48.000]  Então, tô com a Deus, que ele tá tendo esse espaço dele,\n",
            "[00:48.000 --> 00:51.000]  que ele era, que ficou se tornou no amigo,\n",
            "[00:51.000 --> 00:53.000]  também do Zag Brown, que é dizer,\n",
            "[00:53.000 --> 00:56.000]  as coisas estão se encaixando numa maneira muito natural\n",
            "[00:56.000 --> 01:00.000]  desse sentido, então, é o máximo,\n",
            "[01:00.000 --> 01:04.000]  teveendo ele, pra mim é engraçado,\n",
            "[01:04.000 --> 01:06.000]  porque eu vejo ele com um amigo,\n",
            "[01:06.000 --> 01:08.000]  não como um diretor pego,\n",
            "[01:08.000 --> 01:10.000]  e cheio de equipes, o enfim, o meu presidente aqui,\n",
            "[01:10.000 --> 01:12.000]  quando sei qual o título dele,\n",
            "[01:12.000 --> 01:15.000]  mas é engraçado, a gente vai crescendo,\n",
            "[01:15.000 --> 01:17.000]  vai se tornando,\n",
            "[01:17.000 --> 01:20.000]  tem no posições importantes,\n",
            "[01:20.000 --> 01:23.000]  mas tô sempre torcendo por ele, claro.\n",
            "[01:23.000 --> 01:25.000]  E a gente tá junto agora,\n",
            "[01:25.000 --> 01:27.000]  eu estou estando em outro equipe,\n",
            "[01:27.000 --> 01:29.000]  também de um outro lado diferente,\n",
            "[01:29.000 --> 01:31.000]  eu sempre te periou,\n",
            "[01:31.000 --> 01:33.000]  com esse tecilado,\n",
            "[01:33.000 --> 01:34.000]  com o outro brasileiro,\n",
            "[01:34.000 --> 01:36.000]  os fintes pautes, obviamente,\n",
            "[01:36.000 --> 01:39.000]  eu tinha falado muito com tanto com o mesmo,\n",
            "[01:39.000 --> 01:41.000]  com o pie, também,\n",
            "[01:41.000 --> 01:43.000]  tentamos aí eu queria fazer um teste\n",
            "[01:43.000 --> 01:44.000]  para os brasileiros,\n",
            "[01:44.000 --> 01:46.000]  na nossa equipe,\n",
            "[01:46.000 --> 01:48.000]  mas eu também não tenho,\n",
            "[01:48.000 --> 01:50.000]  até um ponto,\n",
            "[01:50.000 --> 01:53.000]  certa força de poder\n",
            "[01:53.000 --> 01:54.000]  mudar isso,\n",
            "[01:54.000 --> 01:56.000]  então, e não conseguir,\n",
            "[01:56.000 --> 01:59.000]  porque eu sei que temos grandes talentes,\n",
            "[01:59.000 --> 02:02.000]  eu acho que o Brasil hoje,\n",
            "[02:02.000 --> 02:03.000]  não são hoje,\n",
            "[02:03.000 --> 02:06.000]  mas a Safra tá voltando novamente,\n",
            "[02:06.000 --> 02:07.000]  e vai acontecer,\n",
            "[02:07.000 --> 02:09.000]  vai ter normalmente pilotos,\n",
            "[02:09.000 --> 02:10.000]  não foi uma insta,\n",
            "[02:10.000 --> 02:11.000]  então tô na hora de ir a insta,\n",
            "[02:11.000 --> 02:12.000]  não posto-leta agora, na forma não,\n",
            "[02:12.000 --> 02:14.000]  que é de, vai voltar,\n",
            "[02:14.000 --> 02:16.000]  é um ciclo que talvez demoram um pouquinho,\n",
            "[02:16.000 --> 02:17.000]  mas,\n",
            "[02:17.000 --> 02:18.000]  e eu acredito que,\n",
            "[02:18.000 --> 02:19.000]  a hora do certo,\n",
            "[02:19.000 --> 02:21.000]  está voltando também,\n",
            "[02:21.000 --> 02:24.000]  com, com empresas brasileiras,\n",
            "[02:24.000 --> 02:26.000]  dando só um momento que o dólar agora\n",
            "[02:26.000 --> 02:27.000]  também é alto,\n",
            "[02:27.000 --> 02:29.000]  mas eu acredito que,\n",
            "[02:29.000 --> 02:30.000]  é,\n",
            "[02:30.000 --> 02:32.000]  por isso que acho que tô a cara aqui no Brasil\n",
            "[02:32.000 --> 02:33.000]  tá tão forte,\n",
            "[02:33.000 --> 02:34.000]  pelo certo sentido,\n",
            "[02:34.000 --> 02:35.000]  tínhamos fiscais,\n",
            "[02:35.000 --> 02:37.000]  enfim, e o talento também,\n",
            "[02:37.000 --> 02:39.000]  mas nós não temos a Safra normalmente,\n",
            "[02:39.000 --> 02:40.000]  e, então,\n",
            "[02:40.000 --> 02:42.000]  tô torcendo também para que os pilotos\n",
            "[02:42.000 --> 02:43.000]  bem um pra cá,\n",
            "[02:43.000 --> 02:44.000]  com o do do ouvido texto aí,\n",
            "[02:44.000 --> 02:46.000]  de pressão muito pessoal aqui da,\n",
            "[02:46.000 --> 02:48.000]  da forma aí,\n",
            "[02:48.000 --> 02:49.000]  de tão isso importante,\n",
            "[02:49.000 --> 02:51.000]  mas a gente tem pelo menos,\n",
            "[02:51.000 --> 02:52.000]  é,\n",
            "[02:52.000 --> 02:55.000]  os detalhes de pilotos brasileiros aparecendo,\n",
            "[02:55.000 --> 02:57.000]  porque uma hora vai pintar o que,\n",
            "[02:57.000 --> 02:58.000]  que vai,\n",
            "[02:58.000 --> 02:59.000]  vai espetar.\n",
            "[02:59.000 --> 03:02.000]  O que você viu do do ouvido,\n",
            "[03:02.000 --> 03:04.000]  se acha que realmente,\n",
            "[03:04.000 --> 03:07.000]  ao fato dele não conseguir chegar\n",
            "[03:07.000 --> 03:08.000]  a indi,\n",
            "[03:08.000 --> 03:10.000]  foi mais uma questão de,\n",
            "[03:10.000 --> 03:12.000]  realmente não ter sempre disponível.\n",
            "[03:15.000 --> 03:16.000]  Não, eu,\n",
            "[03:16.000 --> 03:17.000]  tentei colocar,\n",
            "[03:17.000 --> 03:18.000]  inclusive,\n",
            "[03:18.000 --> 03:20.000]  ele, naquela época,\n",
            "[03:20.000 --> 03:21.000]  o ano passado,\n",
            "[03:21.000 --> 03:22.000]  mas,\n",
            "[03:22.000 --> 03:24.000]  as pessoas também tem que olhar\n",
            "[03:24.000 --> 03:25.000]  em uma maneira que,\n",
            "[03:25.000 --> 03:26.000]  quando você,\n",
            "[03:26.000 --> 03:27.000]  que é fazendo,\n",
            "[03:27.000 --> 03:28.000]  tem que arriscar,\n",
            "[03:28.000 --> 03:29.000]  se tem que fazer arriscado,\n",
            "[03:29.000 --> 03:31.000]  mas tem que ficar,\n",
            "[03:31.000 --> 03:33.000]  pensando na forma não ainda,\n",
            "[03:33.000 --> 03:35.000]  e achando que vai abrir\n",
            "[03:35.000 --> 03:37.000]  uma porta,\n",
            "[03:37.000 --> 03:38.000]  um acento assim,\n",
            "[03:38.000 --> 03:40.000]  sem problema nenhum,\n",
            "[03:40.000 --> 03:41.000]  não tem tão que experiência\n",
            "[03:41.000 --> 03:42.000]  que nos Estados Unidos.\n",
            "[03:42.000 --> 03:43.000]  Então,\n",
            "[03:43.000 --> 03:44.000]  essa, o caro colégate,\n",
            "[03:44.000 --> 03:45.000]  por exemplo,\n",
            "[03:45.000 --> 03:46.000]  o caro colégate está fazendo\n",
            "[03:46.000 --> 03:47.000]  com ele corretíssimo,\n",
            "[03:47.000 --> 03:48.000]  ele começou na Europa,\n",
            "[03:48.000 --> 03:49.000]  mudou de rumo,\n",
            "[03:49.000 --> 03:51.000]  e está criando o nome.\n",
            "[03:51.000 --> 03:52.000]  Está criando o nome,\n",
            "[03:52.000 --> 03:53.000]  esse é um caro,\n",
            "[03:53.000 --> 03:54.000]  que eu acredito também,\n",
            "[03:54.000 --> 03:55.000]  possa muito,\n",
            "[03:55.000 --> 03:56.000]  muito em breve,\n",
            "[03:56.000 --> 03:57.000]  estriar na forma índica.\n",
            "[03:57.000 --> 03:58.000]  Então,\n",
            "[03:58.000 --> 04:01.000]  as pessoas tem que tomar um ali,\n",
            "[04:01.000 --> 04:02.000]  não,\n",
            "[04:02.000 --> 04:03.000]  já está se capulando no lado\n",
            "[04:03.000 --> 04:04.000]  por outro,\n",
            "[04:04.000 --> 04:05.000]  que acaba,\n",
            "[04:05.000 --> 04:06.000]  realmente,\n",
            "[04:06.000 --> 04:07.000]  não superando nada.\n",
            "[04:07.000 --> 04:08.000]  Perfeito.\n",
            "[04:08.000 --> 04:09.000]  E você, na indi,\n",
            "[04:09.000 --> 04:10.000]  como é que,\n",
            "[04:10.000 --> 04:11.000]  o que pode fazer,\n",
            "[04:11.000 --> 04:12.000]  a indi,\n",
            "[04:12.000 --> 04:13.000]  quem desce ano,\n",
            "[04:14.000 --> 04:14.380]  o caro,\n",
            "[04:14.380 --> 04:16.000]  foi de 200 anos.\n",
            "[04:16.000 --> 04:17.280]  E se vai fazer\n",
            "[04:17.280 --> 04:18.980]  com axisos demais\n",
            "[04:18.980 --> 04:19.000]  inteiro,\n",
            "[04:19.000 --> 04:19.980]  você vai focar apenas na,\n",
            "[04:19.980 --> 04:21.500]  na grande corrida.\n",
            "[04:21.500 --> 04:24.980]  Ó apenas na grande corrida,\n",
            "[04:24.980 --> 04:25.620]  não,\n",
            "[04:25.620 --> 04:26.820]  não me faseu lá indios\n",
            "[04:26.820 --> 04:28.000]  pessoal,\n",
            "[04:28.000 --> 04:29.060]  só uma ideia de preparação\n",
            "[04:29.060 --> 04:31.000]  para fora da 10,\n",
            "[04:31.000 --> 04:34.000]  que conselhos filosóficimas sheep industriais,\n",
            "[04:34.000 --> 04:36.000]  e depois lá na semana da corrida,\n",
            "[04:36.000 --> 04:37.000]  à costa façam,\n",
            "[04:37.000 --> 04:39.000]  que eu tinha acermnicos inteira.\n",
            "[04:39.000 --> 04:40.000]  E aí,\n",
            "[04:40.000 --> 04:40.800]  para que oorrido\n",
            "[04:40.800 --> 04:41.000]  então,\n",
            "[04:41.000 --> 04:46.700]  O carro é o mesmo do ano passado, a equipe tem alguns mudanças, alguns integrantes novos,\n",
            "[04:46.700 --> 04:54.560]  esse ano vale a lembrar que a gente pudou de que da parte técnica da R&T por Áganácia,\n",
            "[04:54.560 --> 04:58.660]  então que deu os cálculos e os ingerros que nós vamos usar um ser da Áganácia,\n",
            "[04:58.660 --> 05:03.860]  então a equipe é muito grande, a equipe é muito grande, que é uma equipe que foi campeã ao ano passado,\n",
            "[05:03.860 --> 05:07.040]  aliás, o equipe que seria muito forte de anápolis,\n",
            "[05:07.120 --> 05:12.060]  então a expectativa grande da gente aí está andando no forte.\n",
            "[05:12.900 --> 05:20.340]  Legal, eu falei da do mesmo ai, mas era mais me referindo ao misto,\n",
            "[05:20.340 --> 05:25.020]  que tem aquela corrida antes do misto, e aí os trabalhos, como é o entendido?\n",
            "[05:25.020 --> 05:28.900]  Como é ação nas quintas milhares indenáproas, aliás.\n",
            "[05:28.900 --> 05:34.440]  Quero agradecer demais a sua participação aí, quem sabe a gente nos vê muito mais aí,\n",
            "[05:34.480 --> 05:40.460]  ou neste tocar, ou também, obviamente, na cobertura e nessas quintas milhas indenífonas, muito obrigado.\n",
            "[05:40.960 --> 05:45.440]  Pois é, já falando neste tocar, estou aqui voltando na... não é, se ouvir,\n",
            "[05:45.440 --> 05:49.400]  né, para indo pro aeropuerto, quem sabe, já estou praticando para esse tocar aí,\n",
            "[05:49.400 --> 05:53.340]  vamos ver, vamos torcer a gente, tem uma hora, vai ter que pintar esse tocar.\n",
            "[05:53.340 --> 05:56.800]  Pois é muito mesmo, eles estão no ser desligado por tudo,\n",
            "[05:56.800 --> 06:02.300]  e estão aí na torcida para as quintas milhares de daitona e para as quintas milhas indenáproas,\n",
            "[06:02.300 --> 06:06.140]  esse ano vai ser literalmente duas quintas bilhas.\n",
            "Arquivo de mídia deletado: NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.webm\n",
            "Arquivo transcrito deletado: NÃO TEM QUE PENSAR NA F1： Castroneves MANDA REAL sobre DRUGO na Indy ｜ Felipe segue RESERVA DA ASTON.txt\n",
            "Iniciando o download do arquivo ZIP: transcricoes.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_83b868ac-63a6-4865-bee5-c9042a7e5bd3\", \"transcricoes.zip\", 5635)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O arquivo ZIP 'transcricoes.zip' não será excluído automaticamente.\n",
            "Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Excluir zip remanescente"
      ],
      "metadata": {
        "id": "jYlFgJQgIVbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.remove('transcricoes.zip')"
      ],
      "metadata": {
        "id": "r9rnSnLBIeyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, módulo concluído"
      ],
      "metadata": {
        "id": "4C5CFfEaM9Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Drive - Pasta inteira"
      ],
      "metadata": {
        "id": "qdDLMkWVNAMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Monta o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Caminho da pasta de mídia no Google Drive\n",
        "pasta_midia_drive = \"/content/drive/MyDrive/Igor Whisper/Pasta 02\"\n",
        "# Substitua pelo caminho da sua pasta de mídia no Drive\n",
        "# Pasta 02 designada para isso\n",
        "\n",
        "# Verifica se a pasta de mídia existe\n",
        "if not os.path.exists(pasta_midia_drive):\n",
        "    print(f\"A pasta '{pasta_midia_drive}' não existe no Google Drive.\")\n",
        "else:\n",
        "    # 3. Cria a pasta para as transcrições no Google Drive (caso não exista)\n",
        "    pasta_transcricoes = os.path.join(pasta_midia_drive, \"transcricoes\")\n",
        "    if not os.path.exists(pasta_transcricoes):\n",
        "        os.makedirs(pasta_transcricoes)\n",
        "\n",
        "    # 4. Input para escolher o modelo\n",
        "    print(\"Escolha o modelo do Whisper para transcrição:\")\n",
        "    print(\"1. tiny (mais rápido e menor precisão)\")\n",
        "    print(\"2. base (rápido e boa precisão)\")\n",
        "    print(\"3. small (boa precisão, mais lento)\")\n",
        "    print(\"4. turbo (mais rápido e boa precisão)\")\n",
        "\n",
        "    modelo_escolhido = input(\"Digite o número do modelo desejado (1, 2, 3, 4): \")\n",
        "\n",
        "    modelos = {\n",
        "        \"1\": \"tiny\",\n",
        "        \"2\": \"base\",\n",
        "        \"3\": \"small\",\n",
        "        \"4\": \"turbo\"\n",
        "    }\n",
        "\n",
        "    modelo = modelos.get(modelo_escolhido, \"tiny\")  # Se a opção for inválida, usa o modelo \"tiny\" como padrão.\n",
        "    print(f\"Modelo selecionado: {modelo}\")\n",
        "\n",
        "    # 5. Processa os arquivos de mídia na pasta de mídia no Google Drive\n",
        "    for file_name in os.listdir(pasta_midia_drive):\n",
        "        input_file = os.path.join(pasta_midia_drive, file_name)\n",
        "\n",
        "        # Verifica se é um arquivo de mídia\n",
        "        if file_name.endswith((\".mp4\", \".mkv\", \".avi\", \".mov\", \".mp3\", \".flv\", \".wav\")):\n",
        "            print(f\"Transcrevendo arquivo de mídia: {file_name}\")\n",
        "\n",
        "            # Comando do Whisper para transcrição\n",
        "            output_txt = os.path.join(pasta_transcricoes, file_name.split('.')[0] + \".txt\")\n",
        "            !whisper \"{input_file}\" --model {modelo} --language Portuguese --output_dir \"{pasta_transcricoes}\" --output_format txt\n",
        "\n",
        "            # Verificar se o arquivo de transcrição foi realmente criado\n",
        "            if os.path.exists(output_txt):\n",
        "                print(f\"Arquivo transcrito salvo em: {output_txt}\")\n",
        "            else:\n",
        "                print(f\"Erro: Não foi possível gerar o arquivo de transcrição para {file_name}\")\n",
        "\n",
        "            # Deleta o arquivo de mídia após a transcrição\n",
        "            os.remove(input_file)\n",
        "            print(f\"Arquivo de mídia deletado: {file_name}\")\n",
        "\n",
        "    # 6. Verifica se há transcrições antes de compactar\n",
        "    transcricoes = [f for f in os.listdir(pasta_transcricoes) if f.endswith('.txt')]\n",
        "    if not transcricoes:\n",
        "        print(\"Não há transcrições para compactar.\")\n",
        "    else:\n",
        "        # 7. Compacta as transcrições em um arquivo ZIP\n",
        "        zip_file = \"/content/drive/MyDrive/Igor Whisper/transcricoes.zip\"  # Caminho do arquivo ZIP no Google Drive\n",
        "        with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "            for file_name in transcricoes:\n",
        "                file_path = os.path.join(pasta_transcricoes, file_name)\n",
        "                zipf.write(file_path, arcname=file_name)  # Adiciona ao ZIP\n",
        "                os.remove(file_path)  # Exclui o arquivo após adicioná-lo ao ZIP\n",
        "                print(f\"Arquivo transcrito deletado: {file_name}\")\n",
        "\n",
        "        # 8. Faz o download do arquivo ZIP para o PC (opcional)\n",
        "        print(f\"Iniciando o download do arquivo ZIP: {zip_file}\")\n",
        "        files.download(zip_file)\n",
        "\n",
        "    # 9. Exclusão manual do ZIP (opcional)\n",
        "    print(f\"O arquivo ZIP '{zip_file}' não será excluído automaticamente.\")\n",
        "    print(\"Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "collapsed": true,
        "id": "F3GwySOfRqkO",
        "outputId": "50e25a8c-d08c-4145-c54c-8fb94da3d023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Escolha o modelo do Whisper para transcrição:\n",
            "1. tiny (mais rápido e menor precisão)\n",
            "2. base (rápido e boa precisão)\n",
            "3. small (boa precisão, mais lento)\n",
            "4. turbo (mais rápido e boa precisão)\n",
            "Digite o número do modelo desejado (1, 2, 3, 4): 1\n",
            "Modelo selecionado: tiny\n",
            "Transcrevendo arquivo de mídia: teste 3.mp4\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:08.000]  Nós não poderíamos nos calar e não me quer um M.\n",
            "[00:08.000 --> 00:14.000]  Agora, já deu um minuto já, eu vou dar para o pessoal.\n",
            "[00:14.000 --> 00:15.000]  É tchau.\n",
            "[00:15.000 --> 00:16.000]  É tchau.\n",
            "[00:16.000 --> 00:17.000]  É tchau.\n",
            "[00:17.000 --> 00:18.000]  Bem-sv, é tchau.\n",
            "[00:18.000 --> 00:19.000]  É tchau.\n",
            "[00:19.000 --> 00:20.000]  É tchau.\n",
            "[00:20.000 --> 00:21.000]  É tchau.\n",
            "[00:21.000 --> 00:22.000]  É tchau.\n",
            "[00:22.000 --> 00:23.000]  É tchau.\n",
            "[00:23.000 --> 00:24.000]  É tchau.\n",
            "Arquivo transcrito salvo em: /content/drive/MyDrive/Igor Whisper/Pasta 02/transcricoes/teste 3.txt\n",
            "Arquivo de mídia deletado: teste 3.mp4\n",
            "Arquivo transcrito deletado: teste 3.txt\n",
            "Iniciando o download do arquivo ZIP: /content/drive/MyDrive/Igor Whisper/transcricoes.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5e2e946-cc87-42a1-86df-4a6a3abc90d8\", \"transcricoes.zip\", 338)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O arquivo ZIP '/content/drive/MyDrive/Igor Whisper/transcricoes.zip' não será excluído automaticamente.\n",
            "Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### codigo para uso\n",
        "\n",
        "(não quero zoar o codigo que ta certo)"
      ],
      "metadata": {
        "id": "I0oR0EtTNFgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcrever\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Monta o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Caminho da pasta de mídia no Google Drive\n",
        "pasta_midia_drive = \"/content/drive/MyDrive/Igor Whisper/Pasta 02\"\n",
        "# Substitua pelo caminho da sua pasta de mídia no Drive\n",
        "# Pasta 02 designada para isso\n",
        "\n",
        "# Verifica se a pasta de mídia existe\n",
        "if not os.path.exists(pasta_midia_drive):\n",
        "    print(f\"A pasta '{pasta_midia_drive}' não existe no Google Drive.\")\n",
        "else:\n",
        "    # 3. Cria a pasta para as transcrições no Google Drive (caso não exista)\n",
        "    pasta_transcricoes = os.path.join(pasta_midia_drive, \"transcricoes\")\n",
        "    if not os.path.exists(pasta_transcricoes):\n",
        "        os.makedirs(pasta_transcricoes)\n",
        "\n",
        "    # 4. Input para escolher o modelo\n",
        "    print(\"Escolha o modelo do Whisper para transcrição:\")\n",
        "    print(\"1. tiny (mais rápido e menor precisão)\")\n",
        "    print(\"2. base (rápido e boa precisão)\")\n",
        "    print(\"3. small (boa precisão, mais lento)\")\n",
        "    print(\"4. turbo (mais rápido e boa precisão)\")\n",
        "\n",
        "    modelo_escolhido = input(\"Digite o número do modelo desejado (1, 2, 3, 4): \")\n",
        "\n",
        "    modelos = {\n",
        "        \"1\": \"tiny\",\n",
        "        \"2\": \"base\",\n",
        "        \"3\": \"small\",\n",
        "        \"4\": \"turbo\"\n",
        "    }\n",
        "\n",
        "    modelo = modelos.get(modelo_escolhido, \"tiny\")  # Se a opção for inválida, usa o modelo \"tiny\" como padrão.\n",
        "    print(f\"Modelo selecionado: {modelo}\")\n",
        "\n",
        "    # 5. Processa os arquivos de mídia na pasta de mídia no Google Drive\n",
        "    for file_name in os.listdir(pasta_midia_drive):\n",
        "        input_file = os.path.join(pasta_midia_drive, file_name)\n",
        "\n",
        "        # Verifica se é um arquivo de mídia\n",
        "        if file_name.endswith((\".mp4\", \".mkv\", \".avi\", \".mov\", \".mp3\", \".flv\", \".wav\")):\n",
        "            print(f\"Transcrevendo arquivo de mídia: {file_name}\")\n",
        "\n",
        "            # Comando do Whisper para transcrição\n",
        "            output_txt = os.path.join(pasta_transcricoes, file_name.split('.')[0] + \".txt\")\n",
        "            !whisper \"{input_file}\" --model {modelo} --language Portuguese --output_dir \"{pasta_transcricoes}\" --output_format txt\n",
        "\n",
        "            # Verificar se o arquivo de transcrição foi realmente criado\n",
        "            if os.path.exists(output_txt):\n",
        "                print(f\"Arquivo transcrito salvo em: {output_txt}\")\n",
        "            else:\n",
        "                print(f\"Erro: Não foi possível gerar o arquivo de transcrição para {file_name}\")\n",
        "\n",
        "            # Deleta o arquivo de mídia após a transcrição\n",
        "            os.remove(input_file)\n",
        "            print(f\"Arquivo de mídia deletado: {file_name}\")\n",
        "\n",
        "    # 6. Verifica se há transcrições antes de compactar\n",
        "    transcricoes = [f for f in os.listdir(pasta_transcricoes) if f.endswith('.txt')]\n",
        "    if not transcricoes:\n",
        "        print(\"Não há transcrições para compactar.\")\n",
        "    else:\n",
        "        # 7. Compacta as transcrições em um arquivo ZIP\n",
        "        zip_file = \"/content/drive/MyDrive/Igor Whisper/transcricoes.zip\"  # Caminho do arquivo ZIP no Google Drive\n",
        "        with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "            for file_name in transcricoes:\n",
        "                file_path = os.path.join(pasta_transcricoes, file_name)\n",
        "                zipf.write(file_path, arcname=file_name)  # Adiciona ao ZIP\n",
        "                os.remove(file_path)  # Exclui o arquivo após adicioná-lo ao ZIP\n",
        "                print(f\"Arquivo transcrito deletado: {file_name}\")\n",
        "\n",
        "        # 8. Faz o download do arquivo ZIP para o PC (opcional)\n",
        "        print(f\"Iniciando o download do arquivo ZIP: {zip_file}\")\n",
        "        files.download(zip_file)\n",
        "\n",
        "    # 9. Exclusão manual do ZIP (opcional)\n",
        "    print(f\"O arquivo ZIP '{zip_file}' não será excluído automaticamente.\")\n",
        "    print(\"Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "collapsed": true,
        "outputId": "50e25a8c-d08c-4145-c54c-8fb94da3d023",
        "id": "nblY9GaOVpdO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Escolha o modelo do Whisper para transcrição:\n",
            "1. tiny (mais rápido e menor precisão)\n",
            "2. base (rápido e boa precisão)\n",
            "3. small (boa precisão, mais lento)\n",
            "4. turbo (mais rápido e boa precisão)\n",
            "Digite o número do modelo desejado (1, 2, 3, 4): 1\n",
            "Modelo selecionado: tiny\n",
            "Transcrevendo arquivo de mídia: teste 3.mp4\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:08.000]  Nós não poderíamos nos calar e não me quer um M.\n",
            "[00:08.000 --> 00:14.000]  Agora, já deu um minuto já, eu vou dar para o pessoal.\n",
            "[00:14.000 --> 00:15.000]  É tchau.\n",
            "[00:15.000 --> 00:16.000]  É tchau.\n",
            "[00:16.000 --> 00:17.000]  É tchau.\n",
            "[00:17.000 --> 00:18.000]  Bem-sv, é tchau.\n",
            "[00:18.000 --> 00:19.000]  É tchau.\n",
            "[00:19.000 --> 00:20.000]  É tchau.\n",
            "[00:20.000 --> 00:21.000]  É tchau.\n",
            "[00:21.000 --> 00:22.000]  É tchau.\n",
            "[00:22.000 --> 00:23.000]  É tchau.\n",
            "[00:23.000 --> 00:24.000]  É tchau.\n",
            "Arquivo transcrito salvo em: /content/drive/MyDrive/Igor Whisper/Pasta 02/transcricoes/teste 3.txt\n",
            "Arquivo de mídia deletado: teste 3.mp4\n",
            "Arquivo transcrito deletado: teste 3.txt\n",
            "Iniciando o download do arquivo ZIP: /content/drive/MyDrive/Igor Whisper/transcricoes.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5e2e946-cc87-42a1-86df-4a6a3abc90d8\", \"transcricoes.zip\", 338)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O arquivo ZIP '/content/drive/MyDrive/Igor Whisper/transcricoes.zip' não será excluído automaticamente.\n",
            "Para excluir o arquivo após o download, execute: os.remove('transcricoes.zip')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoriB4gKPI0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Excluir zip remanescente"
      ],
      "metadata": {
        "id": "m6-rz3iHQvpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.remove('transcricoes.zip')"
      ],
      "metadata": {
        "id": "3y2NRg3_Qx0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}